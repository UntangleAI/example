{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept extraction.\n",
    "\n",
    "Concept extraction service aims to extract higher-level, human-friendly concepts from a trained neural network model instead of giving importance to input features (e.g., assigning importance weightings to pixels). For example, an understandable explanation of why an image classifier outputs the label \"zebra\" would ideally relate to concepts such as \"stripes\" rather than a set of particular pixel values.\n",
    "\n",
    "Similarly, consider a classifier trained on images of firefighters. Now if the training data consisted mostly of males wearing slacks and helmets, then the model would assume that being male with a helmet was an important factor to be a firefighter. How would this help us? Well, this would bring out the bias in the training data which has fewer images of females and we could easily rectify that. We can then rank the training data concerning the ‘male’ concept and use that to balance the dataset better.\n",
    "\n",
    "The Concept Extraction tool provides an interpretation of a neural net’s internal state in terms of human-friendly concepts. Concept Extraction uses directional derivatives to quantify the degree to which a user-defined concept is important to a classification result–for example, how sensitive a prediction of “zebra” is to the presence of stripes.\n",
    "\n",
    "For concept extraction, we need at least 50 clear examples of concepts that you are looking for and a model which is likely to have learned the relevant concept. A quick way to figure this out (has the model learned a concept) would be to look at feature attribution/signal estimation for some images concerning that model. The examples should be either direct textures (for dependency on texture properties) or shapes (which should be clearly defined with multiple colors) or entire sub-objects (which should be cropped from the image). Given all of this information, our concept extraction framework finds out the internal part of the neural network which represents this sub-concept and uses that part’s activation output vector to rank any additional or unseen images for presence/absence/likelihood of the concept in those images data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from untangle import UntangleAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant imports for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 Training a CNN for recognizing MNIST dataset\n",
    "\n",
    "This step is optional. If you would like to train a CNN network to recognize MNIST dataset you can refer to [this tutorial](/tutorials/mnist_model_training) which trains a model for 25 epochs and saves the trained weights into lenet_mnist_model.h5.\n",
    "\n",
    "Or you can download the trained weights from [here](https://untanglemodels.s3.amazonaws.com/lenet_mnist_model.h5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from checkpointed weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(torch.nn.Module):\n",
    "    # TODO: This isn't really a LeNet, but we implement this to be\n",
    "    #  consistent with the Evidential Deep Learning paper\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.model = None\n",
    "\n",
    "    def build(self):\n",
    "        lenet_conv = []\n",
    "        lenet_conv += [torch.nn.Conv2d(1,20, kernel_size=(5,5))]\n",
    "        lenet_conv += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_conv += [torch.nn.MaxPool2d(kernel_size=(2,2), stride=2)]\n",
    "        lenet_conv += [torch.nn.Conv2d(20, 50, kernel_size=(5,5))]\n",
    "        lenet_conv += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_conv += [torch.nn.MaxPool2d(kernel_size=(2,2), stride=2)]\n",
    "\n",
    "        lenet_dense = []\n",
    "        lenet_dense += [torch.nn.Linear(4*4*50, 500)]\n",
    "        lenet_dense += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_dense += [torch.nn.Linear(500, 10)]\n",
    "\n",
    "        self.features = torch.nn.Sequential(*lenet_conv)\n",
    "        self.classifier = torch.nn.Sequential(*lenet_dense)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.features(input)\n",
    "        output = output.view(input.shape[0], -1)\n",
    "        # output = output.view(-1, 4*4*50)\n",
    "        output = self.classifier(output)\n",
    "        return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading model checkpoint...\", end=\"\", flush=True)\n",
    "model = LeNet()\n",
    "model.build()\n",
    "\n",
    "module_path = os.path.dirname(os.path.realpath('.'))\n",
    "model_ckpt_path = os.path.join(module_path, 'lenet_mnist_model.h5')\n",
    "\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    ckpt = torch.load(model_ckpt_path)\n",
    "    model.load_state_dict(ckpt)\n",
    "    model = model.cuda()\n",
    "else:\n",
    "    ckpt = torch.load(model_ckpt_path, map_location='cpu')\n",
    "    model.load_state_dict(ckpt)\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create UntangleAI object and define helper functions to generate concepts and to display them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untangle_ai = UntangleAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_concepts(class_i, num_of_concepts, rotate_by_90deg = False):\n",
    "    loader, _ = untangle_ai.load_mnist_per_class(batch_size=1, data_class=class_i)\n",
    "    loader = list(loader) \n",
    "    concept_idx = random.sample(range(len(loader)), num_of_concepts)\n",
    "    concepts = []\n",
    "    for rn in concept_idx:\n",
    "        tensor =  loader[rn][0]\n",
    "        if rotate_by_90deg:\n",
    "            tensor = tensor.permute(0, 1, 3, 2)\n",
    "        concepts.append((class_i + '-' + str(rn), tensor))\n",
    "    random.shuffle(concepts)   \n",
    "    return concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_concept_images(data, rows, fig_size=None):\n",
    "    figure = plt.figure(figsize=fig_size)\n",
    "    num_of_images = len(data)\n",
    "    for index in range(1, num_of_images+1):\n",
    "        plt.subplot(rows, num_of_images/rows, index)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(data[index-1][1].numpy().squeeze(), cmap='gray_r')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try out our first concept, which is circle. In order to extract 50 images of circles, we just pick a random 50 samples from handwritten digit 0 and use that as concept for circle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = generate_concepts('0', 50)\n",
    "show_concept_images(concepts, 5, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function to batch the data so that the operations can be vectorized and optimized for throughput on GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_data(data, batch_size):\n",
    "    img_path, tensors = zip(*data)\n",
    "    batch = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch.append((list(img_path[i : i+batch_size]), torch.cat(tensors[i : i+batch_size], dim=0)))\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_batch  = batchify_data(concepts, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us generate 1000 unseen data points to rank. We randomly sample them from the 0-9 digit classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data = []\n",
    "for i in range(10):\n",
    "    unseen_data += generate_concepts(str(i), 100)\n",
    "random.shuffle(unseen_data)\n",
    "show_concept_images(unseen_data, 25, (16,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxilary data structure to retrive images from ranked list later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data_dict = {}\n",
    "for img_path, tensor in unseen_data:\n",
    "    unseen_data_dict[img_path] = tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make batch of the unseen data which is used for ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data_batch = batchify_data(unseen_data, 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define arguments that is used for learning and ranking concepts and which is passed to untangle API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    mname = 'lenet5'\n",
    "    act_type = 'flatten'\n",
    "    train_data_path = './mnistasjpg/Training_Patterns_0/' # provide path containing concept images\n",
    "    test_data_path = './mnistasjpg/trainingSample/' # probide path containing test images\n",
    "    save_path = 'circle_concept_' # created ranked csv file with this name\n",
    "    batch_size = 16\n",
    "    img_size = (1,28,28)\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now call the `extract_concept_images` API to learn circle concept and rank unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untangle_ai.extract_concept_images(model, concept_batch, unseen_data_batch, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a helper function to retrieve topk and bottomk ranked images based on the concept learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_and_bottom_ranks(filename, topk=20):\n",
    "    df = pd.read_csv(filename)\n",
    "    top20 = []\n",
    "    bottom20 = []\n",
    "    for i in range(topk):\n",
    "        top20.append((df['image_name'].values[i], unseen_data_dict[df['image_name'].values[i]]))\n",
    "        bottom20.append((df['image_name'].values[-1*(i+1)], unseen_data_dict[df['image_name'].values[-1*(i+1)]]))\n",
    "    \n",
    "    return (top20, bottom20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20, bottom20 = get_top_and_bottom_ranks(args.save_path + 'ranked_list.csv')\n",
    "show_concept_images(top20, 5)\n",
    "show_concept_images(bottom20, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected 0's, 6's, and 9's which have cricle as concept in their handwritten digit are top ranked and 4's, 7's, 5's and 1's which lack concept of circle are bottom ranked.\n",
    "\n",
    "Now let us try with another new concpet which is horizontal line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = generate_concepts('1', 75, True)\n",
    "show_concept_images(concepts, 5, None)\n",
    "concept_batch  = batchify_data(concepts, 16)\n",
    "args.save_path = 'horizontal_line_'\n",
    "untangle_ai.extract_concept_images(model, concept_batch, unseen_data_batch, args)\n",
    "top20, bottom20 = get_top_and_bottom_ranks(args.save_path + 'ranked_list.csv')\n",
    "show_concept_images(top20, 5)\n",
    "show_concept_images(bottom20, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, 4s 5s 7s and 1s which have concept of horizontal line are top ranked, 0s. 3s. 2s, 6s and 9s which lack horizontal line concept are bottom ranked.\n",
    "\n",
    "Let us try with one last concept of vertical lines. For this we will sample random images from hand written digit class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts = generate_concepts('1', 50)\n",
    "show_concept_images(concepts, 5, None)\n",
    "concept_batch  = batchify_data(concepts, 16)\n",
    "args.save_path = 'vertical_line_'\n",
    "untangle_ai.extract_concept_images(model, concept_batch, unseen_data_batch, args)\n",
    "top20, bottom20 = get_top_and_bottom_ranks(args.save_path + 'ranked_list.csv')\n",
    "show_concept_images(top20, 5)\n",
    "show_concept_images(bottom20, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we have 1s, 9s are top ranked which have concept of vertical line in their handwritten digit representation and the remaining classes are bottom ranked.\n",
    "\n",
    "## Conclusion\n",
    "Results are very sensitive to the concept images chosen. Handpicked images that are strong representative of concept would improve ranking of unseen data that have high likelihood of having that concept. \n",
    "Also more sophesticated and well trained models that have higher concept sensitivity than a simple model.\n",
    "\n",
    "All the results are stored in csv format, which and be used for further analysis to derive recall rates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
