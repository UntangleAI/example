{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection and uncertainty estimation in Nueral Network Models\n",
    "\n",
    "Most celebrated discoveries of the 20th century are [Heisenberg Uncertainty Principle](https://en.wikipedia.org/wiki/Uncertainty_principle) and [Godel Incompleteness Theorems](https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems) which helped to understand the limitations of physical model and axiomatic mathematical models respectively. In that spirit, it is equally important to understand the limitation of the neural network models as opposed to their capability. Specifically, in a mission-critical scenario like cancer detection or self-driving cars, it is imperative to build trust in the AI models that are used in such applications.\n",
    "We at UntangleAI build tools, as part of Clarity SDK which helps in explaining the decision/evaluation process behind the neural network model and ascertain the quality of such decision/evaluation using uncertainty modeling. It helps to detect outliers, which otherwise would be classified into one of the existing classes due to maximum likelihood principle used to classify the test point. These are the mislabelled data that needs to be excluded from model training. Also, uncertainty modeling helps in identifying uncertain points, for which the model fails to make the correct prediction. These are data points for which human intervention is needed to label them correctly and retrain the model for better decision-making capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello World of Neural Network\n",
    "\n",
    "[MNIST database](http://yann.lecun.com/exdb/mnist/) handwritten digit recognition is analogous to Hello World of Neural Networks. Let us try to understand uncertainty modeling and ranking on this dataset which consists of 70,000 handwritten digits divided into 60,000 training set and 10,000 validation set. This is a multi-class classification with 10 classes recognizing 0 to 9 digits. For any neural network recognizing anything other than the defined classes is infeasible as the possibilities for out of class distributions are infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 Training a CNN for recognizing MNIST dataset\n",
    "\n",
    "This step is optional. If you would like to train a CNN network to recognize MNIST dataset you can refer to [this tutorial](/tutorials/mnist_model_training) which trains a model for 25 epochs and saves the trained weights into lenet_mnist_model.h5.\n",
    "\n",
    "Or you can download the trained weights from [here](https://untanglemodels.s3.amazonaws.com/lenet_mnist_model.h5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Model Uncertainty Statistics for each class\n",
    "\n",
    "During this phase, we take the trained model and training data set batched by class and learn the distribution that model has learned during its training phase for each class and generate uncertainty statistics for each class and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.set_printoptions(precision=8)\n",
    "from untangle import UntangleAI\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from the trained or downloaded checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the same model used for training\n",
    "class LeNet(nn.Module):\n",
    "    # TODO: This isn't really a LeNet, but we implement this to be\n",
    "    #  consistent with the Evidential Deep Learning paper\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.model = None\n",
    "        lenet_conv = []\n",
    "        lenet_conv += [torch.nn.Conv2d(1,20, kernel_size=(5,5))]\n",
    "        lenet_conv += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_conv += [torch.nn.MaxPool2d(kernel_size=(2,2), stride=2)]\n",
    "        lenet_conv += [torch.nn.Conv2d(20, 50, kernel_size=(5,5))]\n",
    "        lenet_conv += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_conv += [torch.nn.MaxPool2d(kernel_size=(2,2), stride=2)]\n",
    "\n",
    "        lenet_dense = []\n",
    "        lenet_dense += [torch.nn.Linear(4*4*50, 500)]\n",
    "        lenet_dense += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_dense += [torch.nn.Linear(500, 10)]\n",
    "\n",
    "        self.features = torch.nn.Sequential(*lenet_conv)\n",
    "        self.classifier = torch.nn.Sequential(*lenet_dense)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.features(input)\n",
    "        output = output.view(input.shape[0], -1)\n",
    "        output = self.classifier(output)\n",
    "        return(output)\n",
    "    \n",
    "model_ckpt_path = 'lenet_mnist_model.h5'\n",
    "model = LeNet()\n",
    "if (torch.cuda.is_available()):\n",
    "    ckpt = torch.load(model_ckpt_path)\n",
    "    model.load_state_dict(ckpt)\n",
    "    model = model.cuda()\n",
    "else:\n",
    "    ckpt = torch.load(model_ckpt_path, map_location='cpu')\n",
    "    model.load_state_dict(ckpt)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define arguments needed to model and rank uncertainty, some of which are used to tweak thresholds like outlier threshold and uncertainty threshold based on the model complexity and knowledge of data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncertaintyArgs:\n",
    "    mname = 'lenet'              # Model name\n",
    "    batch_size = 64              # Batch size for training\n",
    "    num_classes = 10             # Number of classes model is trained to classify\n",
    "    img_size = (1,28,28)         # Input tensor to model\n",
    "    outlier_threshold = -3       # outlier threshold (default: -3 99.7%)\n",
    "    uncertainty_threshold = 0.2  # uncertainty_threshold (default: 0.2)\n",
    "    sigmoid_node = False\n",
    "    data_class = None            # Model uncertainty for all classes\n",
    "    metric = 'similarity'\n",
    "\n",
    "args = UncertaintyArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create required directories to store uncertainty statistics which will be used later to rank or evaluate a test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.dirname(os.path.realpath('.'))\n",
    "proj_path = os.path.abspath(os.path.join(module_path, os.pardir))\n",
    "model_uncrt_data_path = os.path.join(module_path, 'model_uncrt_data/')\n",
    "results_path = os.path.join(module_path, 'results')\n",
    "if(not os.path.exists(model_uncrt_data_path)):\n",
    "    os.makedirs(model_uncrt_data_path)\n",
    "if(not os.path.exists(results_path)):\n",
    "    os.makedirs(results_path)\n",
    "uncertainty_store_path = os.path.join(model_uncrt_data_path, '{}_uncertainty'.format(args.mname))\n",
    "\n",
    "# Create untangle object\n",
    "untangle_ai = UntangleAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call untangle API (model_uncertainty) to learn and store uncertainty statistics. Provide a data loader which loads the training dataset class by class. For MNIST we have provided an API for the same, which is load_mnist_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loader_fun(class_i):\n",
    "    loader, _ = untangle_ai.load_mnist_per_class(batch_size=args.batch_size, data_class=class_i)\n",
    "    return(loader)\n",
    "\n",
    "untangle_ai.model_uncertainty(model, uncertainty_store_path, train_loader_fun, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Ranking of the data by assigning outlier and uncertainty scores\n",
    "\n",
    "Now we use the uncertainty statistics modeled in Step 1 to rank the dataset and ascertain whether the data point is an outlier or uncertain or certain concerning top 2 classes that the model predicts.\n",
    "\n",
    "We use untangle_ai APIs to rank data which returns a CSV containing the scores for each data point and classifying them as certain, uncertain or outlier concerning top 2 model predictions and for the uncertainty statistics collected in Step 1. \n",
    "\n",
    "Ranking is done using untangle API (rank_data). This API expects a data-loader that loads the evaluation dataset and yields (path, tensor) as the output. Typically path is path to the images and tensor is the transformed and normalized torch tensor corresponding to the image. `path` is used as a key to identify the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [str(item) for item in range(args.num_classes)]\n",
    "ID2Name_Map = dict(zip(keys, keys))\n",
    "data_gen = untangle_ai.mnist_data_gen(args.batch_size, results_path, gen_images=True)\n",
    "df = untangle_ai.rank_data(model, data_gen, uncertainty_store_path, results_path, ID2Name_Map, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "pandas dataframe returned from rank data is now sorted by score. As per our threshold score < 0.2 is treated as uncertain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(results_path + '/scores_similarity.csv')\n",
    "df.sort_values(by=['score'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "df_outliers = df[df['decision'] == 'outlier']\n",
    "for i in range(len(df_outliers)):\n",
    "    img = imread(df_outliers['img_path'].values[i])\n",
    "    print(df_outliers.iloc[i])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uncertain = df[df['decision'] == 'uncertain']\n",
    "for i in range(len(df_uncertain))[:3]:\n",
    "    img = imread(df_uncertain['img_path'].values[i])\n",
    "    print(df_uncertain.iloc[i])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "df_certain = df[df['score'] > args.uncertainty_threshold]\n",
    "for i in range(len(df_certain))[:3]:\n",
    "    img = imread(df_certain['img_path'].values[i])\n",
    "    print(df_certain.iloc[i])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
