{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection and uncertainty estimation in Nueral Network Models\n",
    "\n",
    "Most celebrated discoveries of 20th century are [Heisenberg Uncertainty Principle](https://en.wikipedia.org/wiki/Uncertainty_principle) and [Godel Incompleteness Theorems](https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems) which helped to understand limiltations of  physical model and axiomatic mathematical models respectively. In that spirit it is equally important to understand limitation of the neural network models as opposed to their capability. Specifically in a mission critical scenario like cancer detection or self driving cars, it is imperetive to build trust on the AI models that are used in such applications.\n",
    "\n",
    "We at UntangleAI build tools, as part of Clarity SDK which help in explaining the decision/evaluaiton process behind neural network model and acertain the quality of such decision/evalutaion using uncertainty modelling.\n",
    "It helps to detect outliers, which otherwise would be classified into one of the existing classes due to maximum likelihood principle used to classify the test point. These are the mislabelled data that needs to be excluded from model training.\n",
    "Also, uncertainty modelling helps in identifying uncertain points, for which the model fails to make correct prediction. These are data points for which human intervention is needed to label them correctly and retrain the model for better decision making capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection and uncertainty estimation in Nueral Network Models\n",
    "Most celebrated discoveries of 20th century are Heisenberg Uncertainty Principle and Godel Incompleteness Theorems which helped to understand limiltations of physical model and axiomatic mathematical models respectively. In that spirit it is equally important to understand limitation of the neural network models as opposed to their capability.\n",
    "\n",
    "We at UntangleAI build tools, as part of Clarity SDK which help in explaining the decision/evaluaiton process behind neural network model and acertain the quality of such decision/evalutaion using uncertainty modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello World of Neural Network\n",
    "\n",
    "[MNIST database](http://yann.lecun.com/exdb/mnist/) handwritten digit recognition is analogus to Hello World of Neural Networks. Let us try to understand uncertainty modelling and ranking on this dataset which consists of 70,000 handwritten digits divided into 60,000 training set and 10,000 validation set. This is a multi-class classification with 10 classes recognizing 0 to 9 digits. For any neural network recognizing anything other than the defined classes is infeaseable as the possiblities for out of class distributions are infinite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framework and software dependency\n",
    "\n",
    "We are dependent on the following python packages for this tutorial, which are installed as part of requirement when untangle package is installed.\n",
    "\n",
    "  - torch (strict version dependency of 1.0.0)\n",
    "  - torchvision\n",
    "  - numpy\n",
    "  - pandas\n",
    "  - scikit-learn\n",
    "  - scipy\n",
    "  - scikit-image\n",
    "  - Pillow\n",
    "  - h5py\n",
    "  - matplotlib\n",
    "  - astor\n",
    "  - requests\n",
    "  \n",
    "Install untangle package\n",
    "```\n",
    "pip install <package_name>\n",
    "```\n",
    "\n",
    "### Working with our licenses\n",
    "Copy the license.json file to\n",
    "```\n",
    "$USER/.untangle/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-0 Training a CNN for recognizing MNIST dataset\n",
    "\n",
    "This step is optional. If you would like to train a CNN network to recognize MNIST dataset you can refer to [this notebook](./MNIST Training.ipynb) which trains a model for 10 epochs and saves the trained weights into lenet_mnist_model.h5\n",
    "\n",
    "Or you can download the trained weights from [here](https://untanglemodels.s3.amazonaws.com/lenet_mnist_model.h5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Model Uncertainty Statistics for each class\n",
    "\n",
    "During this phase, we take the trained model and training data set batched by class and learn the distribution that model has learned during its training phase for each class and generate uncertainty statistics for each class and save them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "torch.set_printoptions(precision=8)\n",
    "from untangle import UntangleAI\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from the trained or downloaded checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the same model used for training\n",
    "class LeNet(nn.Module):\n",
    "    # TODO: This isn't really a LeNet, but we implement this to be\n",
    "    #  consistent with the Evidential Deep Learning paper\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.model = None\n",
    "        lenet_conv = []\n",
    "        lenet_conv += [torch.nn.Conv2d(1,20, kernel_size=(5,5))]\n",
    "        lenet_conv += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_conv += [torch.nn.MaxPool2d(kernel_size=(2,2), stride=2)]\n",
    "        lenet_conv += [torch.nn.Conv2d(20, 50, kernel_size=(5,5))]\n",
    "        lenet_conv += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_conv += [torch.nn.MaxPool2d(kernel_size=(2,2), stride=2)]\n",
    "\n",
    "        lenet_dense = []\n",
    "        lenet_dense += [torch.nn.Linear(4*4*50, 500)]\n",
    "        lenet_dense += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_dense += [torch.nn.Linear(500, 10)]\n",
    "        lenet_dense += [torch.nn.LogSoftmax(dim=-1)]\n",
    "\n",
    "        self.features = torch.nn.Sequential(*lenet_conv)\n",
    "        self.classifier = torch.nn.Sequential(*lenet_dense)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.features(input)\n",
    "        output = output.view(input.shape[0], -1)\n",
    "        output = self.classifier(output)\n",
    "        return(output)\n",
    "    \n",
    "model_ckpt_path = 'lenet_mnist_model.h5'\n",
    "model = LeNet()\n",
    "if (torch.cuda.is_available()):\n",
    "    ckpt = torch.load(model_ckpt_path)\n",
    "    model.load_state_dict(ckpt)\n",
    "    model = model.cuda()\n",
    "else:\n",
    "    ckpt = torch.load(model_ckpt_path, map_location='cpu')\n",
    "    model.load_state_dict(ckpt)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define argruments needed to model and rank uncertainty, some of which are used to tweak thresholds like outlier threshold and uncertainty threshold based on the model complexity and knowledge of data distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UncertaintyArgs:\n",
    "    mname = 'lenet'              # Model name\n",
    "    batch_size = 64              # Batch size for training\n",
    "    num_classes = 10             # Number of classes model is trained to classify\n",
    "    img_size = (1,28,28)         # Input tensor to model\n",
    "    outlier_threshold = -3       # outlier threshold (default: -3 99.7%)\n",
    "    uncertainty_threshold = 0.2  # uncertainty_threshold (default: 0.2)\n",
    "    sigmoid_node = False\n",
    "    data_class = None            # Model uncertainty for all classes\n",
    "    metric = 'similarity'\n",
    "\n",
    "args = UncertaintyArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create required directories to store uncertainty statistics which will be used later to rank or evaluate a test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.dirname(os.path.realpath('.'))\n",
    "proj_path = os.path.abspath(os.path.join(module_path, os.pardir))\n",
    "model_uncrt_data_path = os.path.join(module_path, 'model_uncrt_data/')\n",
    "results_path = os.path.join(module_path, 'results')\n",
    "if(not os.path.exists(model_uncrt_data_path)):\n",
    "    os.makedirs(model_uncrt_data_path)\n",
    "if(not os.path.exists(results_path)):\n",
    "    os.makedirs(results_path)\n",
    "uncertainty_store_path = os.path.join(model_uncrt_data_path, '{}_uncertainty'.format(args.mname))\n",
    "\n",
    "# Create untangle object\n",
    "untangle_ai = UntangleAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call untangle API (model_uncertainty) to learn and store uncertainty statistics. Provide a data loader which loads the training dataset class by class. For MNIST we have provided an api for the same, which is load_mnist_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loader_fun(class_i):\n",
    "    loader, _ = untangle_ai.load_mnist_per_class(batch_size=args.batch_size, data_class=class_i)\n",
    "    return(loader)\n",
    "\n",
    "untangle_ai.model_uncertainty(model, uncertainty_store_path, train_loader_fun, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Ranking of the data by assigning outlier and uncertainty scores\n",
    "\n",
    "Now we use the uncertainty statistics modelled in Step 1 to rank the dataset and ascertain weather the data point is an outlier or uncertain or certain with respect to top 2 classes that the model predicts.\n",
    "\n",
    "We use untangle_ai APIs to rank data which returns a csv containing the scores for each data point and classifying them as certain, uncertain or outlier with respect to top 2 model predictions and with respect to the uncertainty statistics collected in Step 1. Ranking is done using untangle API (rank_data)\n",
    "\n",
    "This api expects a dataloader that loads the evaluation dataset and yields (path, tensor) as the output. Typically path is path to the images and tensor is the tranformed and normalized torch tensor corresponding to the image. `path` is used as key to identify the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [str(item) for item in range(args.num_classes)]\n",
    "ID2Name_Map = dict(zip(keys, keys))\n",
    "data_gen = untangle_ai.mnist_data_gen(args.batch_size, results_path, gen_images=True)\n",
    "df = untangle_ai.rank_data(model, data_gen, uncertainty_store_path, results_path, ID2Name_Map, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df.sort_values(by=['score'], inplace=True)\n",
    "df[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(10):\n",
    "    img = imread(df['img_path'].values[-1*(i+1)])\n",
    "    print(df['img_path'].values[-1*(i+1)])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
