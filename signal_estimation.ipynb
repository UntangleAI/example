{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation for Signal Estimation and Attribution\n",
    "\n",
    "Some of the most accurate predictive models today are black-box models. This means that it is really hard to understand how they work. Once we use UntangleAI's uncertainty estimation, we would like to know why the model's decision is uncertain regarding those test points. Some relevant questions one could have at this point are\n",
    "- What are the salient features in the input that model is relying upon, to make its decision?\n",
    "- For a given prediction, how important are each values to that prediction?\n",
    "- What are the relative importance given to these features by the model to frame its decision?\n",
    "- How robust is the model for adversarial attacks? Is it possible to change input slightly to get drastically different model decisions?\n",
    "As part of UntangleAI Signal Estimation service, we provide these insights about the model and a way to visualize the decision process of the model for computer vision applications. This service tries to separate signal component from the noise in the training data and learns for each class what signal/features model is learning and what else it discards as noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 Training a CNN for recognizing MNIST dataset\n",
    "\n",
    "This step is optional. If you would like to train a CNN network to recognize MNIST dataset you can refer to [this tutorial](/tutorials/mnist_model_training) which trains a model for 10 epochs and saves the trained weights into lenet_mnist_model.h5.\n",
    "\n",
    "Or you can download the trained weights from [here](https://untanglemodels.s3.amazonaws.com/lenet_mnist_model.h5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Signal Estimation for each class\n",
    "\n",
    "During this phase, we take the trained model and training data set batched by class and estimate signals that model has learned during its training phase for each class and generate signal estimation statistics for each class and save them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "torch.set_printoptions(precision=8)\n",
    "from untangle import UntangleAI\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the model from the trained or downloaded checkpoint file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the same model used for training\n",
    "class LeNet(nn.Module):\n",
    "    # TODO: This isn't really a LeNet, but we implement this to be\n",
    "    #  consistent with the Evidential Deep Learning paper\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.model = None\n",
    "        lenet_conv = []\n",
    "        lenet_conv += [torch.nn.Conv2d(1,20, kernel_size=(5,5))]\n",
    "        lenet_conv += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_conv += [torch.nn.MaxPool2d(kernel_size=(2,2), stride=2)]\n",
    "        lenet_conv += [torch.nn.Conv2d(20, 50, kernel_size=(5,5))]\n",
    "        lenet_conv += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_conv += [torch.nn.MaxPool2d(kernel_size=(2,2), stride=2)]\n",
    "\n",
    "        lenet_dense = []\n",
    "        lenet_dense += [torch.nn.Linear(4*4*50, 500)]\n",
    "        lenet_dense += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_dense += [torch.nn.Linear(500, 10)]\n",
    "\n",
    "        self.features = torch.nn.Sequential(*lenet_conv)\n",
    "        self.classifier = torch.nn.Sequential(*lenet_dense)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.features(input)\n",
    "        output = output.view(input.shape[0], -1)\n",
    "        output = self.classifier(output)\n",
    "        return(output)\n",
    "    \n",
    "model_ckpt_path = 'lenet_mnist_model.h5'\n",
    "model = LeNet()\n",
    "if (torch.cuda.is_available()):\n",
    "    ckpt = torch.load(model_ckpt_path)\n",
    "    model.load_state_dict(ckpt)\n",
    "    model = model.cuda()\n",
    "else:\n",
    "    ckpt = torch.load(model_ckpt_path, map_location='cpu')\n",
    "    model.load_state_dict(ckpt)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define argruments needed for signal estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalEstimationArgs:\n",
    "    mname = 'lenet'\n",
    "    batch_size = 16\n",
    "    num_classes = 10\n",
    "    img_size = (1,28,28)\n",
    "    input_tensor = torch.randn(1,1,28,28) # provide your own input tensor\n",
    "    input_tensor_true = torch.randn(28,28,1) # provide your own true input tensor / ndarray / PIL Image Obj\n",
    "    data_class = None # or `None` to estimate all classes\n",
    "    mode = 'estimate' # one of `estimate`, `attribute`\n",
    "    topk = 1\n",
    "    cmap = 'seismic'\n",
    "    json = False\n",
    "    hm_diff = 'joint'\n",
    "    \n",
    "args = SignalEstimationArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create required directories to store estimated signal statistics which will be used later to attribute signals for a given test point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.dirname(os.path.realpath('.'))\n",
    "proj_path = os.path.abspath(os.path.join(module_path, os.pardir))\n",
    "model_signal_data_path = os.path.join(module_path, 'model_signal_data/')\n",
    "results_path = os.path.join(module_path, 'results')\n",
    "if(not os.path.exists(model_signal_data_path)):\n",
    "    os.makedirs(model_signal_data_path)\n",
    "if(not os.path.exists(results_path)):\n",
    "    os.makedirs(results_path)\n",
    "signal_store_path = os.path.join(model_signal_data_path, '{}_signals'.format(args.mname))\n",
    "\n",
    "\n",
    "# Create untangle object\n",
    "untangle_ai = UntangleAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call untangle API (estimate_signals) to learn and store signal estimation statistics. Provide a data loader which loads the training dataset class by class. For MNIST we have provided an API for the same, which is load_mnist_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_loader_fun(class_i):\n",
    "    loader, _ = untangle_ai.load_mnist_per_class(batch_size=args.batch_size, data_class=class_i)\n",
    "    return(loader)\n",
    "\n",
    "untangle_ai.estimate_signals(model, signal_store_path, train_loader_fun, args = args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Attributing signals for a test point\n",
    "Now we use the signal statistics modeled in Step 1 to attribute and visualize the signals for an input test point.\n",
    "We use untangle_ai API (attribute_signals) to get signal information and visualization concerning top k class of model prediction. We also generate a joint heatmap and differential heatmap images for the top k classes as part of this API.\n",
    "This API works on individual data points one at a time. It takes input_tensor and image_tensor as (Height, Width, Channel) as its shape as inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to visualize signal/feature considered important for a random data point. We visualize heatmap, differential heat map and inverse differential heatmap for the top 3 classes predicted by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "keys = [str(item) for item in range(args.num_classes)]\n",
    "ID2Name_Map = dict(zip(keys, keys))\n",
    "rand_class = random.randint(0, 9)\n",
    "print('Visualizing signal for a data point in class {}'.format(rand_class))\n",
    "rand_class_loader = train_loader_fun(rand_class)\n",
    "for input_tensor, _ in rand_class_loader:\n",
    "    idx = random.randint(0, input_tensor.shape[0]-1)\n",
    "    for topk in range(1, 2): # Heatmap showing signals in favour of top prediction\n",
    "        args.topk = topk   \n",
    "        input_tensor_single = input_tensor[idx][None, :, :, :]\n",
    "        input_tensor_true = input_tensor[idx].permute(1, 2, 0) # expected shape (H,W,C)\n",
    "        out_prefix = os.path.join(results_path, '{}_signals'.format(idx))\n",
    "        untangle_ai.attribute_signals(model, input_tensor_single, input_tensor_true, signal_store_path,\n",
    "            ID2Name_Map, args, out_prefix)\n",
    "        for i in range(topk):\n",
    "            img = imread(out_prefix + '_class_{}.JPEG'.format(i))\n",
    "            print('Visualizing {}'.format(out_prefix + '_class_{}.JPEG'.format(i)))\n",
    "            plt.imshow(img)\n",
    "            plt.show()\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentally, what we provide is a gradient-based technique. This implies that it will work only with machine learning models which are differentiable (neural networks, logistic regression, and SVMs). The raw feature importance which we compute is comprised of two types of gradients:\n",
    "- Positive Gradients (Red-colored pixels with intensity indicative of positive importance for seismic colormap) Positive gradients indicate which pixels in an image are indicative of evidence in favor of the network's decision/output.\n",
    "- Negative Gradients (Blue colored pixels with intensity indicative of negative importance for seismic colormap) Negative gradients indicate which pixels in an image are indicative of evidence against the network's decision/output.\n",
    "When equal number pixels in an area are comprised of blue and red, then they meaningfully cancel out which should indicate no importance in that entire area.\n",
    "### Expectation:\n",
    "Well trained networks will have feature attributions mostly comprised of positive gradients alone. This means that the network is solely relying on the presence of features to make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topk in range(2, 4): # Joint and Differential Heatmaps comparing top class with other k-1 top predictions.\n",
    "    args.topk = topk   \n",
    "    out_prefix = os.path.join(results_path, '{}_signals'.format(idx))\n",
    "    untangle_ai.attribute_signals(model, input_tensor_single, input_tensor_true, signal_store_path,\n",
    "            ID2Name_Map, args, out_prefix)\n",
    "    for i in range(1, topk):\n",
    "        img = imread(out_prefix + '_diff_class_{}.JPEG'.format(i))\n",
    "        print('Visualizing {}'.format(out_prefix + '_diff_class_{}.JPEG'.format(i)))\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        img = imread(out_prefix + '_invDiff_class_{}.JPEG'.format(i))\n",
    "        print('Visualizing {}'.format(out_prefix + '_invDiff_class_{}.JPEG'.format(i)))\n",
    "        plt.imshow(img)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative Heatmaps\n",
    "\n",
    "\n",
    "- Joint Heatmap\n",
    "\n",
    "If you want to look at evidence for or against a prediction towards a single output node, then a simple heatmap is sufficient. However, when there are multiple classes, these heatmaps need to be compared correctly. This can be challenging because the gradients which are returned at the end of any gradient-based technique are within different orders of magnitude.\n",
    "\n",
    "We provide a modified heatmap function in case you want to compare the attributions of two or more classes which takes into account the relative magnitudes of the gradients of both the classes and applies the heatmap function appropriately to bring all of the values onto the same scale. This is what is described by the joint heatmap terminology. \n",
    "\n",
    "This is the reason why we suggest using top k > 1 for all use cases since this provides us a clearer picture of why class X was chosen and not class Y.\n",
    "\n",
    "- Difference Heatmaps\n",
    "\n",
    "Since with the concept of joint heatmaps, the gradients obtained for the two classes are identical, you can treat them as two 2-D vectors.\n",
    "\n",
    "Difference heatmaps tell you which pixels of class X are important only for class X and not for class Y. (Given logit of X > logit of Y)\n",
    "\n",
    "Similarly, in the reverse direction, inverse difference heatmaps tell you which pixels of class Y are important only for class Y and not for class X.\n",
    "\n",
    "Therefore, the difference heatmaps and the inverse difference heatmaps combined tell a larger story of what are the differentiating pixels/feature maps(if any) for the classifier to differentiate between two classes or does it only care about specific pixel intensities to differentiate between the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from untangle.external.signal_api import _attribute\n",
    "signal = _attribute(model, input_tensor_single, signal_store_path, rand_class, args)\n",
    "signal = signal.permute(0, 2, 3, 1)\n",
    "signal = signal.numpy()\n",
    "heatmap_signal = untangle_ai.get_heatmap(signal)\n",
    "def slider_tool(th):\n",
    "    fig,axes = plt.subplots(1,1,figsize = (8,8))\n",
    "\n",
    "    analysis = [heatmap_signal<th]*heatmap_signal\n",
    "\n",
    "    axes.set_title('Attribution')\n",
    "    axes.imshow(analysis[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "interact(slider_tool, th = (0.0, 0.99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slider tool to highlight pixel importance\n",
    "\n",
    "Since the output of our heatmap function returns a heatmap image, it is sometimes hard to differentiate between the highly important pixels and low importance pixels, especially if high importance pixels cluster around a low importance pixels or vice versa. Slider tools provide a way to find the most important pixels by getting rid of the lowest importance pixels and retaining the highest importance pixels for threshold in the range (0.0, 0.99). Any modification to these high importance features should result in a drastic drop of prediction probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
