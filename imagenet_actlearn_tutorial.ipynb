{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning (re-training) Tutorial tutorial on Restricted ImageNet (20 classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1] Dataset Description and objective definition\n",
    "The `imagenet20_data` is constructed from the ImageNet data using the following classes:\n",
    "* `sports car`, `streetcar`, `car wheel`, `great white shark`, `tiger shark`, `coral reef`, `snorkel`, `bell pepper`, `Granny Smith`, `tiger cat`, `Egyptian cat`, `baseball`, `aircraft carrier`, `broom`, `electric guitar`, `police van`, `school bus`, `sunglasses`, `screwdriver`, `strawberry`\n",
    "\n",
    "* In addition, we partition the training data to construct validation data by taking 300 random images from each class. The data distribution (and directory structure) is as follows:\n",
    "    * `imagenet20_data/train`: 1000 images per category (total 20k images)\n",
    "    * `imagenet20_data/valid`: 300 images per category (total 6k images)\n",
    "    * `imagenet20_data/test`: 50 images per category (total 1k images). This is taken from the original validation data of ILSVRC2012\n",
    "\n",
    "\n",
    "* We want to demonstrate the effect of uncertainty modelling on model performance. Specifically, we find the most meaningful unseen examples, that, when added to the training dataset, result in largest performance improvement compared to random selection. This re-training strategy can be classified into three cases:\n",
    "    * `uncertainty sampling`: Find the most uncertain examples in the unseen dataset (in our case the `valid` dataset), put it back in the training data and re-train model from scratch.\n",
    "    * `random sampling`: Select random samples from the unseen dataset (in our case the `valid` dataset), put it back in the training and re-train model from scratch.\n",
    "    * `inverse uncertainty sampling` (for comparison): Find the least uncertain (highly confident) examples in the `valid` dataset, put it back in the training data and re-train model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2] Load necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "torch.set_printoptions(precision=8)\n",
    "from untangle import UntangleAI\n",
    "untangle_ai = UntangleAI()\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "import shutil\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "from datetime import datetime\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "\n",
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_torch(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3] Provide necessary arguments to perform active learning.\n",
    "\n",
    "* `mname` [`<class str>`] - base name / prefix to generate visualization files\n",
    "* `batch_size` [`<class int>`, `default:64`] - batch size to train model for accuracy and rank unseen data for uncertainty. We observed that a Tesla K80, 12Gb GPU can accommodate a batch size of `64` for a standard (224,224) RGB image for accuracy training.\n",
    "* `num_classes` [`<class int>`] - number of classes in the dataset\n",
    "* `sample_selection` [`<class float>`] - train-validation split of the training data. Since we use the `valid` data folder as the unseen data, we create a split from the training data for validation\n",
    "* `mode` [`<class str>`, default:uncertain] - unseen sample selection from the unseen data folder (in our case the `valid` data folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    # choices: `vgg16_imgnet20_uncertain_retrain`, `vgg16_imgnet20_random_retrain`, `vgg16_imgnet20_certain_retrain`\n",
    "    mname = 'vgg16_imgnet20_uncertain_retrain'\n",
    "    data_name = 'imagenet20_data'\n",
    "    batch_size = 64\n",
    "    num_classes = 20\n",
    "    img_size = (3,224,224)\n",
    "    metric = 'prob' # for future additions. Current default set to `prob`\n",
    "    num_epochs = 300\n",
    "    patience = 10\n",
    "    lr = 0.001\n",
    "    sample_selection = 0.2 # 20% of unseen data\n",
    "    is_al = True\n",
    "    mode = 'uncertain' # one of `uncertain`, `certain`, `random`\n",
    "    sigmoid_node = False # if network contains sigmoid node at last layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4] Define a torch model. Here, we modify VGG16 from torchvision to output 20 probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "class CustomVgg16(torch.nn.Module):\n",
    "    def __init__(self, num_classes=10, use_pretrained=False):\n",
    "        super(CustomVgg16, self).__init__()\n",
    "        base_model = vgg16(pretrained=use_pretrained)\n",
    "\n",
    "        modules = list(base_model.children())\n",
    "        self.features = torch.nn.Sequential(*modules[:-1])\n",
    "\n",
    "        classifier = list(modules[-1].children())[:-1]\n",
    "        classifier += [torch.nn.Linear(4096, num_classes)]\n",
    "        self.classifier = torch.nn.Sequential(*classifier)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.features(input)\n",
    "        output = output.view(input.shape[0], -1)\n",
    "        output = self.classifier(output)\n",
    "        return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [5] Define data / models paths and class name to index mapping\n",
    "\n",
    "\n",
    "* `dataset_path`: Since we will me manipulating the dataset folder by moving images from unseen (`valid`) folder to the `train` folder, we create a copy of the dataset so as to not corrupt the original data\n",
    "* `al_validation_path`: path to save model training and re-training history (loss and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "module_path = os.path.abspath('')\n",
    "data_path = os.path.join(module_path, 'data')\n",
    "src_data_path = os.path.join(data_path, 'imagenet20_data')\n",
    "curr_time_path = 'imagenet20_data_' + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "dataset_path = os.path.join(data_path, curr_time_path) # copy of the data\n",
    "al_validation_path = os.path.join(module_path, '{}_history.pkl'.format(args.mname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [6] Load torch model and define transforms to be applied to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomVgg16(num_classes=args.num_classes, use_pretrained=False)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "in_transform = transforms.Compose([\n",
    "    transforms.Resize((args.img_size[1]+32, args.img_size[2]+32)),\n",
    "    transforms.CenterCrop((args.img_size[1], args.img_size[2])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [7] Define utility functions to be used to move data to across directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_labelled_data_pool(img_list, dst_data_path):\n",
    "    '''\n",
    "    Move a list of images to a specified path\n",
    "    :param img_list: list of image paths\n",
    "    :param dst_data_path: destination directory to move images to\n",
    "    :return: None\n",
    "    '''\n",
    "    for img_i_path in img_list:\n",
    "        target = img_i_path.split('/')[-2].strip()\n",
    "        dst_img_dir = os.path.join(dst_data_path, str(target))\n",
    "\n",
    "        if(not os.path.exists(dst_img_dir)):\n",
    "            os.makedirs(dst_img_dir)\n",
    "        shutil.move(img_i_path, dst_img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_random_examples(src_data_path, dst_data_path, num_examples=1000):\n",
    "    '''\n",
    "    Move random `num_examples` number of images from source dir to destination dir\n",
    "    :param src_data_path: source directory containing images\n",
    "    :param dst_data_path: destination directory containing images\n",
    "    :param num_examples: number of images to move\n",
    "    :return: None\n",
    "    '''\n",
    "    \n",
    "    subdirs = [x[0] for x in os.walk(src_data_path)][1:]\n",
    "    num_classes = len(subdirs)\n",
    "    sample_counts = [int(num_examples / num_classes) for _ in range(num_classes)]\n",
    "    offset = num_examples - sum(sample_counts)\n",
    "    for idx, val in enumerate([1] * offset):\n",
    "        sample_counts[idx] += val\n",
    "\n",
    "    for subdir, sample_count in zip(subdirs, sample_counts):\n",
    "        subdir_name = subdir.split('/')[-1].strip()\n",
    "        all_files = os.listdir(subdir)\n",
    "        sampled_files = random.sample(all_files, sample_count)\n",
    "        src_subdir_path = os.path.join(src_data_path, subdir_name)\n",
    "        dst_subdir_path = os.path.join(dst_data_path, subdir_name)\n",
    "        if(not os.path.exists(dst_subdir_path)):\n",
    "            os.makedirs(dst_subdir_path)\n",
    "\n",
    "        for fname in sampled_files:\n",
    "            src_file = os.path.join(src_subdir_path, fname)\n",
    "            dst_file = os.path.join(dst_subdir_path, fname)\n",
    "            shutil.move(src_file, dst_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [8] Create a copy of the dataset and get data_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"cp -r {} {}\".format(src_data_path, dataset_path))\n",
    "train_path = os.path.join(dataset_path, 'train')\n",
    "pool_path = os.path.join(dataset_path, 'valid')\n",
    "test_path = os.path.join(dataset_path, 'test')  # custom created: half of validation data selected at random\n",
    "base_model_path = os.path.join(module_path, 'models')\n",
    "if(not os.path.exists(base_model_path)):\n",
    "    os.makedirs(base_model_path)\n",
    "model_ckpt_path = os.path.join(base_model_path, '{}.pth'.format(args.mname))\n",
    "\n",
    "train_loader, val_loader = untangle_ai.load_from_dir(train_path, batch_size=args.batch_size, transform=in_transform,\n",
    "                                                  shuffle=False, get_paths=False,\n",
    "                                                  val_split=True)  # val_split shuffles automatically\n",
    "test_loader, n_images = untangle_ai.load_from_dir(test_path, batch_size=args.batch_size, transform=in_transform,\n",
    "                                     shuffle=False, get_paths=False, get_img_count=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [9] Define training loop for model training with learning rate schedule at patience as follows:\n",
    "\n",
    "* When no reduction is validation loss for `patience` number of epochs, reduce learning rate by `0.2` (users can configure it to a suitable number as desired)\n",
    "* When no reduction in validation loss for `5 x patience` number of epochs, then we early stop the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience, save_path, msg):\n",
    "        self.patience = patience\n",
    "        self.save_path = save_path\n",
    "        self.min_loss = 100000.0 # some high number\n",
    "        self.min_acc = 0.0 # some low number\n",
    "        self.curr_patience = 0\n",
    "        self.optimal_params = None # deepcopy(model.state_dict())\n",
    "        self.msg = msg\n",
    "\n",
    "    def __call__(self, curr_loss, model):\n",
    "        if(curr_loss < self.min_loss):\n",
    "            self.min_loss = curr_loss\n",
    "            self.optimal_params = deepcopy(model.state_dict())\n",
    "            self.curr_patience = 0\n",
    "            return(False)\n",
    "        else:\n",
    "            self.curr_patience += 1\n",
    "            if(self.curr_patience == self.patience):\n",
    "                print(self.msg)\n",
    "                if(self.save_path is not None):\n",
    "                    torch.save(model.state_dict(), self.save_path)\n",
    "                    print(\"Model saved in path: {}\".format(self.save_path))\n",
    "                return(True)\n",
    "\n",
    "    def reset_patience(self):\n",
    "        self.curr_patience = 0\n",
    "\n",
    "class AdjustLR:\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def __call__(self, optimizer, decay_rate=0.2):\n",
    "        print(\"Updating Learning Rate from [{}] to [{}]\".format(self.lr, self.lr*decay_rate))\n",
    "        self.lr *= decay_rate\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = self.lr * decay_rate\n",
    "\n",
    "def train_with_patience(base_model, train_loader, val_loader, test_loader, lr=0.001, max_epochs=50,\n",
    "           patience=10, print_freq=40, save_model_path=None):\n",
    "    base_model.train()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(base_model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    patience_reached = EarlyStopping(patience=patience, save_path=save_model_path, msg='Patience Reached! Reducing LR!!')\n",
    "    early_stopping = EarlyStopping(patience=5*patience, save_path=save_model_path, msg='No update since {} epochs. EarlyStopping!!'.format(5*patience))\n",
    "    adjustLR = AdjustLR(lr)\n",
    "    break_flag = False\n",
    "\n",
    "    acc_history = dict()\n",
    "    acc_history['train'] = []\n",
    "    acc_history['val'] = []\n",
    "    acc_history['test'] = []\n",
    "    loss_history = dict()\n",
    "    loss_history['train'] = []\n",
    "    loss_history['val'] = []\n",
    "    loss_history['test'] = []\n",
    "    for epoch in range(max_epochs):\n",
    "        torch.cuda.empty_cache()\n",
    "        if(not break_flag):\n",
    "            for mode in ['train', 'val']:\n",
    "                running_loss = 0.0; running_corrects = 0.0; counts = 0\n",
    "                if(mode == 'train'):\n",
    "                    base_model.train()\n",
    "                    data_loader = train_loader\n",
    "                else:\n",
    "                    base_model.eval()\n",
    "                    data_loader = val_loader\n",
    "\n",
    "                for step, (input_tensor, target) in enumerate(data_loader):\n",
    "                    input_tensor = input_tensor.cuda()\n",
    "                    target = target.cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    output = base_model(input_tensor)\n",
    "                    if(isinstance(output, tuple)):\n",
    "                        output = output[-1] # in case of multi output, take the last (pre-sfotmax) output\n",
    "                    pred = torch.max(output, 1)[1]\n",
    "                    model_loss = criterion(output, target)\n",
    "                    if(mode == 'train'):\n",
    "                        model_loss.backward()\n",
    "                        optimizer.step()\n",
    "                    running_loss += model_loss.item() * input_tensor.size(0)\n",
    "                    running_corrects += torch.sum(pred == target.data).item()\n",
    "                    counts += input_tensor.size(0)\n",
    "\n",
    "                    if(mode == 'train' and step % print_freq == 0):\n",
    "                        print(\n",
    "                            \"Epoch: [{epoch}][{curr_step}/{total}] \\t\"\n",
    "                            \"train_loss: {loss} \\t\"\n",
    "                            \"train_acc: {acc}\".format(\n",
    "                                epoch=epoch,\n",
    "                                curr_step=step+1,\n",
    "                                total=len(train_loader),\n",
    "                                loss=running_loss / counts,\n",
    "                                acc=running_corrects / counts))\n",
    "\n",
    "                epoch_loss = running_loss / counts\n",
    "                epoch_acc = running_corrects / counts\n",
    "                acc_history[mode].append(epoch_acc)\n",
    "                loss_history[mode].append(epoch_loss)\n",
    "                print(\"[{}]: loss: {} \\t acc: {}\".format(mode.upper(), epoch_loss, epoch_acc))\n",
    "\n",
    "                if(mode == 'val'):\n",
    "                    if(patience_reached(epoch_loss, base_model)):\n",
    "                        base_model.load_state_dict(patience_reached.optimal_params)\n",
    "                        adjustLR(optimizer, decay_rate=0.25)\n",
    "\n",
    "                    if(early_stopping(epoch_loss, base_model)):\n",
    "                        break_flag = True\n",
    "\n",
    "    # Validation conf matrix\n",
    "    base_model.load_state_dict(early_stopping.optimal_params)\n",
    "    _, _, val_conf_matrix = get_pred(base_model, val_loader, optimizer, criterion)\n",
    "    epoch_loss, epoch_acc, test_conf_matrix = get_pred(base_model, test_loader, optimizer, criterion)\n",
    "    acc_history['test'].append(epoch_acc)\n",
    "    loss_history['test'].append(epoch_loss)\n",
    "    print(\"[TEST]: loss: {} \\t acc: {}\".format(epoch_loss, epoch_acc))\n",
    "    if(save_model_path is not None):\n",
    "        torch.save(base_model.state_dict(), save_model_path)\n",
    "    return(base_model, acc_history, loss_history, test_conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [10] Train model for a given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, acc, loss, conf_matrix = trainer.train_with_patience(base_model=model, train_loader=train_loader,\n",
    "                                                                    val_loader=val_loader, test_loader=test_loader,\n",
    "                                                                    lr=args.lr, max_epochs=args.num_epochs,\n",
    "                                                                    patience=args.patience, print_freq=50,\n",
    "                                                                    save_model_path=model_ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [11] **Uncertainty Sampling (using our method)**: Select relevant examples from the unseen data (`valid` directory) to add back to the training dataset. \n",
    "\n",
    "* We use our uncertainty modelling and ranking method to find out the most uncertain examples, which, when added to the training data, result in highest accuracy jump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying model for additional instances and re-training...\n",
      "Estimating uncertainty for class: `0`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.40s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `0`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.41s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `1`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.38s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `1`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.40s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `2`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.50s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `2`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:12<00:00,  4.51s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `3`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.43s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `3`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.43s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `4`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.47s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `4`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.48s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `5`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.47s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `5`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.46s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `6`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:12<00:00,  4.52s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `6`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:12<00:00,  4.52s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `7`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.44s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `7`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.43s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `8`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.48s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `8`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.47s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `9`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:12<00:00,  4.53s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `9`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:12<00:00,  4.53s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `10`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:12<00:00,  4.52s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `10`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:12<00:00,  4.51s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `11`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.40s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `11`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.39s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `12`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.49s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `12`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.47s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `13`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.42s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `13`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.43s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `14`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:12<00:00,  4.53s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `14`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:12<00:00,  4.55s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `15`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.42s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `15`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.41s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `16`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.46s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `16`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.46s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `17`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.42s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `17`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:10<00:00,  4.42s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `18`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.45s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `18`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:11<00:00,  4.44s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating uncertainty for class: `19`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.61s/it]\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating neighbour statistics for class: `19`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [01:13<00:00,  4.60s/it]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'UntangleAI' object has no attribute 'ImgPathAndTensor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2dcd7e0d3362>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Rank Uncertainty on unseen dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0munlabelled_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muntangle_ai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImgPathAndTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_transform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# returns dataset that generates img_paths and img_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabelled_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mranking_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muntangle_ai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muncertainty_store_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_out_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mID2Name_Map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mID2Name_Map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'UntangleAI' object has no attribute 'ImgPathAndTensor'"
     ]
    }
   ],
   "source": [
    "uncertainty_base_path = os.path.join(module_path, 'model_uncertainty_data')\n",
    "if(not os.path.exists(uncertainty_base_path)):\n",
    "    os.makedirs(uncertainty_base_path)\n",
    "uncertainty_store_path = os.path.join(uncertainty_base_path, '{}_uncertainty'.format(args.mname))\n",
    "num_query_instances = int(args.sample_selection * n_images) + 1 # no. of examples to select from the unseen dataset\n",
    "print(\"querying model for (additional) uncertain instances...\")\n",
    "\n",
    "# Model Uncertainty on training dataset\n",
    "def train_loader_fun(class_i):\n",
    "    loader = untangle_ai.load_from_dir_per_class(train_path, data_class=class_i, batch_size=args.batch_size,\n",
    "                                     transform=in_transform, shuffle=False, get_paths=False)\n",
    "    return(loader)\n",
    "model.eval()\n",
    "untangle_ai.model_uncertainty(model, uncertainty_store_path, train_loader_fun, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ranking data uncertainty...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/94 [00:00<?, ?it/s]\n",
      "  0%|          | 0/19 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/94 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DistanceEstimator' object has no attribute 'inv_conv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-947bb875075a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0munlabelled_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImgPathAndTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0min_transform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# returns dataset that generates img_paths and img_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdata_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munlabelled_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mranking_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muntangle_ai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muncertainty_store_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_out_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mID2Name_Map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mID2Name_Map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mranking_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranking_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mranking_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mranking_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_query_instances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/untangle/__init__.py\u001b[0m in \u001b[0;36mrank_data\u001b[0;34m(self, base_model, data_gen, act_store_path, base_out_path, ID2Name_Map, args)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrank_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_store_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_out_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mID2Name_Map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0muncrt_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_store_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_out_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mID2Name_Map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_softmax_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/untangle/external/uncrt_api.py\u001b[0m in \u001b[0;36mrank_data\u001b[0;34m(base_model, data_gen, stats_store_path, base_out_path, ID2Name_Map, args)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mbatch_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_uncertainty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mID2Name_Map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_store_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muncrt_dict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0muncrt_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'img_path'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_path\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/untangle/uncertainty_estimator/evaluator.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mevaluator.eval_uncertainty\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/untangle/uncertainty_estimator/evaluator.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mevaluator.DistanceEstimator.build\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/untangle/uncertainty_estimator/evaluator.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mevaluator.DistanceEstimator.__compute_c2c_shared_bhattacharya_dist\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DistanceEstimator' object has no attribute 'inv_conv'"
     ]
    }
   ],
   "source": [
    "class ImgPathAndTensor(datasets.ImageFolder):\n",
    "    def __getitem__(self, index):\n",
    "        # this is what ImageFolder normally returns\n",
    "        original_tuple = super(ImgPathAndTensor, self).__getitem__(index)\n",
    "        # the image file path\n",
    "        path = self.imgs[index][0]\n",
    "        # make a new tuple that includes original and the path\n",
    "        tuple_with_path = (path, original_tuple[0])\n",
    "        return tuple_with_path\n",
    "\n",
    "# Rank Uncertainty on unseen dataset\n",
    "keys = [str(item) for item in range(args.num_classes)]\n",
    "ID2Name_Map = dict(zip(keys, keys)) # class index to class name mapping. Defaults to index as names \n",
    "unlabelled_set = ImgPathAndTensor(pool_path, transform=in_transform) # returns dataset that generates img_paths and img_tensors\n",
    "data_loader = torch.utils.data.DataLoader(unlabelled_set, batch_size=args.batch_size, shuffle=False, num_workers=1)\n",
    "ranking_df = untangle_ai.rank_data(model, data_loader, uncertainty_store_path, base_out_path=None, ID2Name_Map=ID2Name_Map, args=args)\n",
    "ranking_df = ranking_df.sort_values(by=['score'], ascending=True)\n",
    "ranking_df = ranking_df.head(num_query_instances)\n",
    "uncertain_examples = list(ranking_df['img_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [12] Re-train model from scratch using the given training data + additional uncertain examples\n",
    "\n",
    "* Merge training data directory with additional examples\n",
    "* Retrain model from scratch on this new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_labelled_data_pool(uncertain_examples, train_path)\n",
    "\n",
    "train_loader, val_loader = untangle_ai.load_from_dir(train_path, batch_size=args.batch_size, transform=in_transform,\n",
    "                                                  shuffle=False, get_paths=False,\n",
    "                                                  val_split=True)  # val_split shuffles automatically\n",
    "test_loader, n_images = untangle_ai.load_from_dir(test_path, batch_size=args.batch_size, transform=in_transform,\n",
    "                                     shuffle=False, get_paths=False, get_img_count=True)\n",
    "\n",
    "model = CustomVgg16(num_classes=args.num_classes, use_pretrained=False) # reset model for re-training\n",
    "model = model.to(DEVICE)\n",
    "_, retrain_acc, retrain_loss, retrain_conf_matrix = train_with_patience(base_model=model, train_loader=train_loader,\n",
    "                                                                  val_loader=val_loader, test_loader=test_loader,\n",
    "                                                                  lr=0.1*args.lr, max_epochs=args.num_epochs,\n",
    "                                                                  patience=args.patience, print_freq=50,\n",
    "                                                                  save_model_path=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [13] Clear up copy of the dataset and save the history is a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.exists(dataset_path)):\n",
    "    shutil.rmtree(dataset_path)\n",
    "\n",
    "validation_metric = dict()\n",
    "validation_metric['acc'] = [acc, retrain_acc]\n",
    "validation_metric['loss'] = [loss, retrain_loss]\n",
    "validation_metric['cm'] = [conf_matrix, retrain_cm]\n",
    "with open(al_validation_path, 'wb') as F:\n",
    "    pickle.dump(validation_metric, F, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [14] Print training statistics, performance curves and improvements in confusion metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(al_validation_path, 'rb'))\n",
    "idx2Name_map = {'0' : 'base', '1' : 'retrain', '2' : 'diff'}\n",
    "\n",
    "base_df = pd.DataFrame(np.array([data['acc'][0]['train'],\n",
    "                              data['acc'][0]['val'],\n",
    "                              data['loss'][0]['train'],\n",
    "                              data['loss'][0]['val']]).T,\n",
    "                    columns=[\"train acc\", \"val acc\", \"train loss\", \"val loss\"])\n",
    "retrain_df = pd.DataFrame(np.array([data['acc'][1]['train'],\n",
    "                              data['acc'][1]['val'],\n",
    "                              data['loss'][1]['train'],\n",
    "                              data['loss'][1]['val']]).T,\n",
    "                    columns=[\"train acc\", \"val acc\", \"train loss\", \"val loss\"])\n",
    "\n",
    "fig = plt.figure()\n",
    "sn.set(color_codes=True)\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "sn.lineplot(data=base_df, x=base_df.index, y='train acc')\n",
    "sn.lineplot(data=retrain_df, x=retrain_df.index, y='train acc')\n",
    "ax.legend(loc='bottom right', labels=['base', 're-train'])\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "sn.lineplot(data=base_df, x=base_df.index, y='val acc')\n",
    "sn.lineplot(data=retrain_df, x=retrain_df.index, y='val acc')\n",
    "ax.legend(loc='bottom right', labels=['base', 're-train'])\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "sn.lineplot(data=base_df, x=base_df.index, y='train loss')\n",
    "sn.lineplot(data=retrain_df, x=retrain_df.index, y='train loss')\n",
    "ax.legend(loc='top right', labels=['base', 're-train'])\n",
    "ax = fig.add_subplot(2, 2, 4)\n",
    "sn.lineplot(data=base_df, x=base_df.index, y='val loss')\n",
    "sn.lineplot(data=retrain_df, x=retrain_df.index, y='val loss')\n",
    "ax.legend(loc='top right', labels=['base', 're-train'])\n",
    "performance_save_path = \"{}_performance.pdf\".format(mname)\n",
    "plt.savefig(performance_save_path)\n",
    "print(\"performance saved in path: {}\".format(performance_save_path))\n",
    "\n",
    "cm_save_path = \"{}_confusion_matrix.pdf\".format(mname)\n",
    "with PdfPages(cm_save_path) as pdf_pages:\n",
    "    for idx in range(len(idx2Name_map)):\n",
    "        if(idx == 2):\n",
    "            in_data = (data['cm'][1] - data['cm'][0]) * np.eye(data['cm'][1].shape[0], data['cm'][1].shape[1])\n",
    "        else:\n",
    "            in_data = data['cm'][idx]\n",
    "        df_cm = pd.DataFrame(in_data, index = [str(i) for i in range(args.num_classes)],\n",
    "                          columns = [str(i) for i in range(args.num_classes)])\n",
    "        fig = plt.figure(figsize = (10,7))\n",
    "        sn.heatmap(df_cm, annot=True, cmap='jet')\n",
    "        pdf_pages.savefig(fig)\n",
    "print(\"confusion metric saved in path: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(performance_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
