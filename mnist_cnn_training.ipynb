{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple classifer inheriting from torch.nn.module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    # TODO: This isn't really a LeNet, but we implement this to be\n",
    "    #  consistent with the Evidential Deep Learning paper\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.model = None\n",
    "        lenet_conv = []\n",
    "        lenet_conv += [torch.nn.Conv2d(1,20, kernel_size=(5,5))]\n",
    "        lenet_conv += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_conv += [torch.nn.MaxPool2d(kernel_size=(2,2), stride=2)]\n",
    "        lenet_conv += [torch.nn.Conv2d(20, 50, kernel_size=(5,5))]\n",
    "        lenet_conv += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_conv += [torch.nn.MaxPool2d(kernel_size=(2,2), stride=2)]\n",
    "\n",
    "        lenet_dense = []\n",
    "        lenet_dense += [torch.nn.Linear(4*4*50, 500)]\n",
    "        lenet_dense += [torch.nn.ReLU(inplace=True)]\n",
    "        lenet_dense += [torch.nn.Linear(500, 10)]\n",
    "        lenet_dense += [torch.nn.LogSoftmax(dim=-1)]\n",
    "\n",
    "        self.features = torch.nn.Sequential(*lenet_conv)\n",
    "        self.classifier = torch.nn.Sequential(*lenet_dense)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.features(input)\n",
    "        output = output.view(input.shape[0], -1)\n",
    "        output = self.classifier(output)\n",
    "        return(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define train and test function which is used in training the model. This performs the necessary forward and backward propogation to adjust weights of various nureons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in tqdm(enumerate(train_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define arguments needed for training which can be used to tweak the training process like learning rate,\n",
    "number of epochs etc... Also we will define tranform function applied on input images. Please ensure that the\n",
    "same transform function is used throughout the whole process of training, uncertainty modelling and ranking of\n",
    "the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingArgs:\n",
    "    batch_size = 64\n",
    "    test_batch_size = 1000\n",
    "    epochs = 10\n",
    "    lr = 0.01\n",
    "    seed = 1\n",
    "    momentum = 0.5\n",
    "    log_interval = 300\n",
    "\n",
    "transformFn=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us train the model for 10 epochs and save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model():\n",
    "    args = TrainingArgs()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./lenet-mnist-data', train=True, download=True, transform=transformFn),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./lenet-mnist-data', train=False, transform=transformFn),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "    model = LeNet().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch)\n",
    "        test(args, model, device, test_loader)\n",
    "\n",
    "    torch.save(model.state_dict(), \"lenet_mnist_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 10 epochs and save the model\n",
    "train_and_save_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
